---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 5m
  chart:
    spec:
      # renovate: registryUrl=https://prometheus-community.github.io/helm-charts
      chart: kube-prometheus-stack
      version: 13.5.0
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
      interval: 5m
  timeout: 20m
  values:
    # defaultRules:
    #   rules:
    #     kubeApiserverAvailability: true
    #     kubeApiserver: true
    server:
      resources:
        requests:
          memory: 1500Mi
          cpu: 25m
        limits:
          memory: 2000Mi
    prometheusOperator:
      createCustomResource: true
      # prometheusConfigReloaderImage:
      #   repository: quay.io/coreos/prometheus-config-reloader
      #   tag: v0.39.0
      # configmapReloadImage:
      #   repository: jimmidyson/configmap-reload
      #   tag: v0.4.0
      configReloaderCpu: 200m
    alertmanager:
      alertmanagerSpec:
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: rook-ceph-block
              resources:
                requests:
                  storage: 10Gi
        tolerations:
        - key: "arm"
          operator: "Exists"
        podMetadata:
          annotations:
            backup.velero.io/backup-volumes: alertmanager-kube-prometheus-stack-alertmanager-db
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "nginx"
          nginx.ingress.kubernetes.io/auth-url: "https://auth.eviljungle.com/oauth2/auth"
          nginx.ingress.kubernetes.io/auth-signin: https://auth.eviljungle.com/oauth2/start
        hosts: [prom-alert.eviljungle.com]
        tls:
        - hosts:
          - prom-alert.eviljungle.com
      config:
        global:
          resolve_timeout: 5m
        route:
          # group_by: ['alertname', 'job']
          # group_wait: 30s
          # group_interval: 5m
          # repeat_interval: 6h
          receiver: 'slack-notifications'
          # receiver: 'pagerduty'
          routes:
            - match:
                alertname: Watchdog
              receiver: 'null'
            - receiver: 'pagerduty'
              match:
                severity: critical
              continue: true
            - receiver: 'slack-notifications'
        inhibit_rules:
        - source_match:
            severity: 'critical'
          target_match:
            severity: 'warning'
          # Apply inhibition if the alertname is the same.
          equal: ['alertname', 'namespace']
        templates: ["*.tmpl"]
      templateFiles:
        pagerduty-custom.tmpl: |-
          {{- define "pagerduty.custom.description" -}}[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if ne .CommonAnnotations.summary ""}}{{ .CommonAnnotations.summary }} {{ else if ne .CommonAnnotations.message ""}}{{ .CommonAnnotations.message }} {{ else if ne .CommonAnnotations.description ""}}{{ .CommonAnnotations.description }} {{ else }}{{ .CommonLabels.alertname }}{{ end }}{{- end -}}
    nodeExporter:
      serviceMonitor:
        relabelings:
        - sourceLabels: [__meta_kubernetes_pod_node_name]
          targetLabel: nodename
          targetLabel: instance
          action: replace
    kubelet:
      serviceMonitor:
        metricRelabelings:
        - action: replace
          sourceLabels:
          - __meta_kubernetes_pod_node_name
          targetLabel: instance
    grafana:
      enabled: false
    kubeEtcd:
      enabled: false
    kubeControllerManager:
      enabled: false
      # endpoints:
      # - 10.2.0.30
    kubeScheduler:
      enabled: false
      # endpoints:
      # - 10.2.0.30
    kubeProxy:
      enabled: false
    prometheus-node-exporter:
      tolerations:
      - key: "arm"
        operator: "Exists"
      - key: "armhf"
        operator: "Exists"
      - key: "node-role.kubernetes.io/master"
        operator: "Exists"
    prometheus:
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "nginx"
          nginx.ingress.kubernetes.io/auth-url: "https://auth.eviljungle.com/oauth2/auth"
          nginx.ingress.kubernetes.io/auth-signin: https://auth.eviljungle.com/oauth2/start
        hosts: [prom-server.eviljungle.com]
        tls:
        - hosts:
          - prom-server.eviljungle.com
      prometheusSpec:
        # image:
        #   repository: quay.io/prometheus/prometheus
        #   tag: v2.20.0
        replicas: 2
        replicaExternalLabelName: "replica"
        ruleSelector: {}
        ruleNamespaceSelector: {}
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelector: {}
        serviceMonitorNamespaceSelector: {}
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelector: {}
        podMonitorNamespaceSelector: {}
        podMonitorSelectorNilUsesHelmValues: false
        retention: 6h
        enableAdminAPI: true
        walCompression: true
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: rook-ceph-block
              resources:
                requests:
                  storage: 10Gi
        # tolerations:
        # - key: "arm"
        #   operator: "Exists"
        podMetadata:
          annotations:
            backup.velero.io/backup-volumes: prometheus-kube-prometheus-stack-prometheus-db
        thanos:
          image: quay.io/thanos/thanos:v0.17.0
          version: v0.17.0
          objectStorageConfig:
            name: thanos
            key: object-store.yaml
  valuesFrom:
  - kind: Secret
    name: "kube-prometheus-stack-helm-values"
    optional: false
