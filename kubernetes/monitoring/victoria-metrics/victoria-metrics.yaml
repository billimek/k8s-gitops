---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/helmrelease-helm-v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: vm
  namespace: monitoring
spec:
  chart:
    spec:
      chart: victoria-metrics-k8s-stack
      version: 0.42.0
      sourceRef:
        kind: HelmRepository
        name: victoriametrics-charts
        namespace: flux-system
  interval: 1h
  maxHistory: 3
  install:
    crds: CreateReplace
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    crds: CreateReplace
    cleanupOnFail: true
    remediation:
      retries: 3
  uninstall:
    keepHistory: false
  values:
    fullnameOverride: stack
    victoria-metrics-operator:
      enabled: true
      env:
        - name: VM_VMALERTDEFAULT_CONFIGRELOADERCPU
          value: 0
        - name: VM_VMAGENTDEFAULT_CONFIGRELOADERCPU
          value: 0
        - name: VM_VMALERTMANAGER_CONFIGRELOADERCPU
          value: 0
      operator:
        disable_prometheus_converter: false # Ensure we keep enabled the converter to sync prom rules to VM rules
        enable_converter_ownership: true # Required to allow VM to remove VM rules it imports if a prometheus rule is deleted

    defaultDashboards:
      enabled: true
      grafanaOperator:
        # -- Create dashboards as CRDs (requires grafana-operator to be installed)
        enabled: false

    defaultRules:
      create: true

    vmsingle:
      enabled: true
      spec:
        # -- Data retention period. Possible units character: h(ours), d(ays), w(eeks), y(ears), if no unit character specified - month. The minimum retention period is 24h. See these [docs](https://docs.victoriametrics.com/single-server-victoriametrics/#retention)
        retentionPeriod: "1w" # TODO: change this to "3" once the old cluster is removed
        replicaCount: 1
        storage:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 15Gi # TODO: change this back to 60Gi once the old cluster is removed
        resources:
          limits:
            memory: 2Gi
          requests:
            cpu: 530m
            memory: 1.2Gi

      ingress:
        enabled: true
        ingressClassName: nginx-tailscale
        hosts:
        - vm.t.eviljungle.com # TODO: change this once the old cluster is removed
        tls:
        - hosts:
          - vm.t.eviljungle.com # TODO: change this once the old cluster is removed

    alertmanager:
      enabled: true
      spec:
        externalURL: "https://vm-alert.t.eviljungle.com" # TODO: change this once the old cluster is removed
      ingress:
        enabled: true
        ingressClassName: nginx-tailscale
        hosts:
        - vm-alert.t.eviljungle.com # TODO: change this once the old cluster is removed
        tls:
        - hosts:
          - vm-alert.t.eviljungle.com # TODO: change this once the old cluster is removed
      config:
        route:
          group_by: ["alertname", "job"]
          group_wait: 45s
          group_interval: 10m
          repeat_interval: 12h
          routes:
          - matchers:
            - alertname=~"WatchDog|InfoInhibitor|KubeMemoryOvercommit|CephNodeNetworkPacketDrops"
            receiver: 'blackhole'
          - matchers:
            - severity=~"warning|critical"
            receiver: slack-notifications
        inhibit_rules:
          - source_matchers:
              - severity = "critical"
            target_matchers:
              - severity = "warning"
            equal: ["alertname", "namespace"]
        receivers:
        - name: 'blackhole'
        - name: 'slack-notifications'
          slack_configs:
            - channel: '#notifications'
              icon_url: https://avatars3.githubusercontent.com/u/3380462
              username: 'Alertmanager - SANDBOX' # TODO: change this once the old cluster is removed
              send_resolved: true
              title: |-
                [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ if eq .Status "firing" }}{{ if eq .CommonLabels.severity "critical" }}üî•{{ else if eq .CommonLabels.severity "warning" }}‚ö†Ô∏è{{ else }}‚ÑπÔ∏è{{ end }}{{ else }}‚úÖ{{ end }} {{ if ne .CommonAnnotations.summary ""}}{{ .CommonAnnotations.summary }} {{ else if ne .CommonAnnotations.message ""}}{{ .CommonAnnotations.message }} {{ else if ne .CommonAnnotations.description ""}}{{ .CommonAnnotations.description }} {{ else }}{{ .CommonLabels.alertname }}{{ end }}
              text: |-
                {{ range .Alerts -}}
                  **Alert:** {{ .Annotations.title }}{{ if .Labels.severity }} - `{{ .Labels.severity }}`{{ end }}

                {{ if ne .Annotations.summary ""}}**Summary:** {{ .Annotations.summary }} {{ else if ne .Annotations.message ""}}**Message:** {{ .Annotations.message }} {{ else if ne .Annotations.description ""}}**Description:** {{ .Annotations.description }}{{ end }}

                üè∑ Labels:
                   {{ range .Labels.SortedPairs }} ‚Ä¢ *{{ .Name }}:* `{{ .Value }}`
                   {{ end }}

                  {{- if .Annotations.runbook_url }}{{ "\n" }} ‚Ä¢ [*runbook*]({{ .Annotations.runbook_url }}){{ end -}}
                  {{- if .GeneratorURL }}{{ "\n" }} ‚Ä¢ [*source*]({{ .GeneratorURL }}){{ end }}
                {{ end }}


    vmalert:
      enabled: true
      spec:
        extraArgs:
          external.url: "https://vmalert.t.eviljungle.com" # TODO: change this once the old cluster is removed
      ingress:
        enabled: true
        ingressClassName: nginx-tailscale
        hosts:
        - vmalert.t.eviljungle.com # TODO: change this once the old cluster is removed
        tls:
        - hosts:
          - vmalert.t.eviljungle.com # TODO: change this once the old cluster is removed

    vmagent:
      enabled: true
      ingress:
        enabled: true
        ingressClassName: nginx-tailscale
        hosts:
        - vmagent.t.eviljungle.com # TODO: change this once the old cluster is removed
        tls:
        - hosts:
          - vmagent.t.eviljungle.com # TODO: change this once the old cluster is removed
      spec:
        # TODO: re-enable this once home-assistant is running on the new cluster
        # additionalScrapeConfigs:
        #   name: victoria-metrics-k8s-stack-helm-values
        #   key: additionalScrapeConfigs.yaml 
        inlineScrapeConfig: |
          - job_name: 'ipmi'
            metrics_path: '/metrics'
            static_configs:
            - targets:
              - nas.home:9290
          - job_name: 'node'
            static_configs:
            - targets:
              - proxmox-b.home:9100
              - proxmox-c.home:9100
              - opnsense.home:9100
              - nas.home:9100
          - job_name: 'smartctl'
            metrics_path: '/metrics'
            static_configs:
            - targets:
              - nas.home:9633
          - job_name: 'upsc-exporter'
            metrics_path: '/metrics'
            static_configs:
            - targets:
              - proxmox-b.home:8081
              - proxmox-c.home:8081
          - job_name: 'zfs'
            metrics_path: '/metrics'
            static_configs:
            - targets:
              - nas.home:9134
        resources:
          limits:
            cpu: null
            memory: 500Mi
          requests:
            cpu: 500m
            memory: 100Mi

    grafana:
      enabled: false

    prometheus-node-exporter:
      enabled: true

    kube-state-metrics:
      enabled: true

    kubelet:
      enabled: true
      vmScrape:
        spec:
          # drop high cardinality label and useless metrics for cadvisor and kubelet
          metricRelabelConfigs:
          # Drop less useful container CPU metrics.
          - sourceLabels: [__name__]
            action: drop
            regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
          # Drop less useful / always zero container memory metrics.
          - sourceLabels: [__name__]
            action: drop
            regex: 'container_memory_(failures_total|mapped_file|swap)'
          # Drop less useful container process metrics.
          - sourceLabels: [__name__]
            action: drop
            # regex: 'container_(file_descriptors|tasks_state|threads_max)'
            regex: 'container_(tasks_state|threads_max)'
          # Drop less useful container filesystem metrics.
          - sourceLabels: [__name__]
            action: drop
            regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
          # Drop less useful container blkio metrics.
          - sourceLabels: [__name__]
            action: drop
            regex: 'container_blkio_device_usage_total'
          # Drop container spec metrics that overlap with kube-state-metrics.
          - sourceLabels: [__name__]
            action: drop
            regex: 'container_spec.*'
          # Drop cgroup metrics with no pod.
          - sourceLabels: [id, pod]
            action: drop
            regex: '.+;'
          - action: drop
            sourceLabels: [__name__]
            regex: prober_probe_duration_seconds_bucket
          # Drop high-cardinality labels.
          - action: labeldrop
            regex: (uid|id|name|pod_uid|interface)
          - action: drop
            sourceLabels: [__name__]
            regex: (rest_client_request_duration_seconds_bucket|rest_client_request_duration_seconds_sum|rest_client_request_duration_seconds_count)

    kubeApiServer:
      enabled: true

    kubeControllerManager:
      enabled: true
      vmScrape:
        spec:
          endpoints:
            - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
              port: http-metrics
              scheme: https
              tlsConfig:
                caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                insecureSkipVerify: true
                serverName: localhost

    coreDns:
      enabled: true

    kubeEtcd:
      enabled: false
      # service:
      #   selector:
      #     component: kube-apiserver

    kubeScheduler:
      enabled: true
      vmScrape:
        spec:
          endpoints:
            - bearerTokenFile: /var/run/secrets/kubernetes.io/serviceaccount/token
              port: http-metrics
              scheme: https
              tlsConfig:
                caFile: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
                insecureSkipVerify: true
                serverName: localhost

    kubeProxy:
      enabled: false

  valuesFrom:
  - kind: Secret
    name: "victoria-metrics-k8s-stack-helm-values"
    optional: false
